{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[nltk_data] Downloading package punkt to /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "# System \n",
    "#from __future__ import print_function\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "# Learning & Extraction\n",
    "from sklearn.decomposition import NMF\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "import langid\n",
    "from topia.termextract import tag\n",
    "from topia.termextract import extract\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "\n",
    "# Dimensionality Reduction & Distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######\n",
    "# Utility Functions\n",
    "######\n",
    "### Data Preprocessing\n",
    "def remove_numericals(s):\n",
    "    s = \"\".join([c for c in s if not c.isdigit()])\n",
    "    return s\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    s2 = ''\n",
    "    for c in s:\n",
    "        if c not in string.punctuation:\n",
    "            s2 = s2 + c\n",
    "        else:\n",
    "            s2 = s2 + ' '\n",
    "    return s2\n",
    "\n",
    "def remove_propers_POS(s):\n",
    "    tagged = nltk.pos_tag(s.split()) #use NLTK's part of speech tagger\n",
    "    non_propernouns = [word for word,pos in tagged if pos != 'NNP' and pos != 'NNPS']\n",
    "    return ''.join([n + \" \" for n in non_propernouns])\n",
    "\n",
    "def remove_html(s):\n",
    "    s = re.sub(r'<.+?>', ' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "### Tokenizers\n",
    "\n",
    "def tokenizer(s):\n",
    "    return nltk.word_tokenize(s)\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "\n",
    "def stem_tokenizer(s):\n",
    "    return stem_tokens(nltk.word_tokenize(s))\n",
    "\n",
    "os.chdir(os.getcwd())\n",
    "\n",
    "def safe_detect(x):\n",
    "    try:\n",
    "        return(langid.classify(x))[0]\n",
    "    except:\n",
    "        print x\n",
    "        return(None)\n",
    "\n",
    "def preprocessor(s):\n",
    "    s = remove_html(s)\n",
    "    s = remove_propers_POS(s)\n",
    "    s = remove_numericals(s)\n",
    "    s = remove_punctuation(s)\n",
    "    s = s.lower()\n",
    "    #s = spell_check(s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #### FOR REMOVING\n",
    "version = \"_07_07_2016\"\n",
    "with open('amis_articles{0}.jsonl'.format(version)) as f:\n",
    "    articles = pd.DataFrame(json.loads(line) for line in f)\n",
    "\n",
    "articles['date'] = pd.to_datetime(articles['date'])\n",
    "articles['timestamp'] = articles['date'].apply(lambda d: time.mktime(d.timetuple()))\n",
    "articles = articles.sort('date', ascending=1)\n",
    "\n",
    "articles['raw_article'] = articles['article'] \n",
    "\n",
    "sources = list(articles['source'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_topics = 50\n",
    "n_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article        126602\n",
       "date           126602\n",
       "link           126602\n",
       "source         126602\n",
       "title          126602\n",
       "timestamp      126602\n",
       "raw_article    126602\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles['article'] = articles['raw_article'].apply(lambda x: preprocessor(x.decode('utf-8')))\n",
    "# if s is not np.nan and s != ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = text.CountVectorizer(max_df=.95, min_df=2, ngram_range=(1, 1),\n",
    "                                        max_features=n_features, tokenizer=tokenizer,\n",
    "                                        stop_words=list(text.ENGLISH_STOP_WORDS))\n",
    "tf = tf_vectorizer.fit_transform(features)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_freqs = [(word, tf.getcol(idx).sum()) for word, idx in tf_vectorizer.vocabulary_.items()]\n",
    "tf_freqs = pd.DataFrame(tf_freqs, columns=('word', 'freq'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "reconstruction_error = pd.DataFrame(index=np.arange(0, 5*10*11+1), columns = ('n_components', 'alpha', 'l1_ratio', 'reconstruction_error'))\n",
    "n = 0\n",
    "a = 0\n",
    "l = 0\n",
    "replace = 0\n",
    "\n",
    "for c in range(10,60,10) + range(60,200,20) + range(200,400,50):\n",
    "    if os.path.isfile('models/nmf_c'+str(c)+'.pkl') and not replace==1:\n",
    "        nmf_curr = joblib.load('models/nmf_c'+str(c)+'.pkl')\n",
    "    else:\n",
    "        nmf_curr = NMF(n_components=c, random_state=1, alpha=a, l1_ratio=l).fit(tf)\n",
    "        joblib.dump(nmf_curr, 'models/nmf_c'+str(c)+'.pkl')\n",
    "    reconstruction_error.loc[n] = [c, a, l, nmf_curr.reconstruction_err_]\n",
    "    n+=1\n",
    "             \n",
    "joblib.dump(reconstruction_error, 'models/nmf_reconstruction_error.pkl')\n",
    "reconstruction_error = joblib.load('models/nmf_reconstruction_error.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf100_topics=list()\n",
    "nmf100_labels=list()\n",
    "\n",
    "nmf100_user_topics = nmf100.components_ * tf.transpose()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf100.components_):\n",
    "    print(\"Topic #%d: \" % topic_idx + \" \".join([tf_feature_names[i] for i in topic.argsort()[:-51:-1]]))\n",
    "    nmf100_topics.append(\"Topic #%d: \" % topic_idx + \" \".join([tf_feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    nmf100_labels.append(\" \".join([tf_feature_names[x] for x in topic.argsort()[-3:]]))\n",
    "    \n",
    "nmf100_labels = np.asarray(nmf100_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saved_words = 100\n",
    "topic_components = pd.DataFrame(index=np.arange(0, n_topics*saved_words), columns = ('id_topic', 'word_rank', 'text', 'weight'))\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf100.components_):\n",
    "  sorted_topics = topic.argsort()[:-n_words - 1:-1] \n",
    "  n = 0\n",
    "  for i in sorted_topics[0:saved_words]:\n",
    "    topic_components.loc[n_words*(topic_idx)+n]  = [topic_idx, n+1, tf_feature_names[i], topic[i]]\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_index = 10000\n",
    "n_index = 10000\n",
    "user_dist = np.matrix(1 - cosine_similarity(tf[start_index:start_index+n_index])).round(2)\n",
    "np.fill_diagonal(user_dist, np.inf)\n",
    "user_dist[np.where(user_dist <.01)] = np.inf\n",
    "user_min_dist = np.where(user_dist == user_dist.min())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The two closest samples have a cosine similarity of ' + str(user_dist.min()))\n",
    "print('')\n",
    "print('About Me for user 1: ' + str(df_about['dim_about_me'][about_index[start_index + user_min_dist[0]]]))\n",
    "print('')\n",
    "print('About Me for user 2: ' + str(df_about['dim_about_me'][about_index[start_index + user_min_dist[1]]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_dist = np.matrix(1 - cosine_similarity(nmf100.components_))\n",
    "topic_min_dist = np.where(topic_dist == topic_dist[np.where(topic_dist > 0.01)].min())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('The two closest samples have a cosine similarity of ' + str(topic_dist[np.where(topic_dist >.6)].min()))\n",
    "print('')\n",
    "print('About Me for topic 1: ' + str(nmf100_topics[topic_min_dist[0]]))\n",
    "print('')\n",
    "print('About Me for topic 2: ' + str(nmf100_topics[topic_min_dist[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linkage_matrix = ward(topic_dist) # define the linkage_matrix using ward clustering pre-computed distances\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 60)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"left\", labels=np.array(nmf100_labels), leaf_font_size=16);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# uncomment below to save figure\n",
    "plt.savefig('topic_heirarchy.png', dpi=200) #save figure as ward_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
