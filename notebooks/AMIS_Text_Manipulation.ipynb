{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/mrpozzi/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap, cm\n",
    "from matplotlib.path import Path\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "from pandas import DataFrame, Series\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version = \"\"\n",
    "version = \"_07_07_2016\"\n",
    "with open('amis_articles{0}.jsonl'.format(version)) as f:\n",
    "    articles = pd.DataFrame(json.loads(line) for line in f)\n",
    "\n",
    "articles['date'] = pd.to_datetime(articles['date'])\n",
    "articles['timestamp'] = articles['date'].apply(lambda d: time.mktime(d.timetuple()))\n",
    "articles = articles.sort('date', ascending=1)\n",
    "\n",
    "articles['raw_article'] = articles['article'] \n",
    "\n",
    "sources = list(articles['source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# process the articles\n",
    "def clean_and_tokenize_article(article):\n",
    "    tokenized_article = word_tokenize(article)\n",
    "    tokenized_article = [regex.sub(u'', token).lower() for token in tokenized_article]\n",
    "    tokenized_article = filter(lambda x: not x in stopwords.words('english') + [u''], tokenized_article)\n",
    "    return tokenized_article\n",
    "\n",
    "articles['article'] = articles['raw_article'].apply(clean_and_tokenize_article)\n",
    "articles['article'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_sentiment(article, sid = SentimentIntensityAnalyzer()):\n",
    "    sentences = nltk.tokenize.sent_tokenize(article)\n",
    "    cumulative = {'compound': 0.0, 'neg': 0.0, 'neu': 0.0, 'pos': 0.0}\n",
    "    for sentence in sentences:\n",
    "        ss = sid.polarity_scores(sentence)\n",
    "        for key in cumulative.keys():\n",
    "            cumulative[key] += ss[key]\n",
    "    for key in cumulative.keys():\n",
    "        cumulative[key] /= len(sentences)\n",
    "    return cumulative\n",
    "\n",
    "articles['sentiment'] = articles['raw_article'].apply(define_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compound': 0.4404, 'neg': 0.0, 'neu': 0.923, 'pos': 0.077}"
      ]
     },
     "execution_count": 14,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}